{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three major areas\n",
    "* Feature extraction and engineering\n",
    "* Feature scaling\n",
    "* Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction and engineering\n",
    "\"Coming up with features is difficult, time-consuming, requires expert knowledge. 'Applied Machine Learning' is basically feature engineering.\" - Prof. Andrew Ng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Feature Engineering?\n",
    "* **Better representation of data**: Features are basically various representations of the underlying raw data. These representations can be better understood by Machine Learning algorithms. Besides this, we can also often easily visualize these representations. A simple example would be to visualize the frequent word occurrences of a newspaper article as opposed to being totally perplexed as to what to do with the raw text!\n",
    "* **Better performing models**: The right features tend to give models that outperform other models no matter how complex the algorithm is. In general if you have the right feature set, even a simple model will perform well and give desired results. In short, better features make better models.\n",
    "* **Essential for model building and evaluation**: We have mentioned this numerous times by now, raw data cannot be used to build Machine Learning models. Get your data, extract features, and start building models! Also on evaluating model performance and tuning the models, you can reiterate over your feature set to choose the right set of features to get the best model.\n",
    "* **More flexibility on data types**: While is it definitely easier to use numeric data types directly with Machine Learning algorithms with little or no data transformations, the real challenge is to build models on more complex data types like text, images, and even videos. Feature engineering helps us build models on diverse data types by applying necessary transformations and enables us to work even on complex unstructured data.\n",
    "* **Emphasis on the business and domain**: Data scientists and analysts are usually busy in processing, cleaning data and building models as a part of their day to day tasks. This often creates a gap between the business stakeholders and the technical/analytics team. Feature engineering involves and enables data scientists to take a step back and try to understand the domain and the business better, by taking valuable inputs from the business and subject matter experts. This is necessary to create and select features that might be useful for building the right model to solve the problem. Pure statistical and mathematical knowledge is rarely sufficient to solve a complex real-world problem. Hence feature engineering emphasizes to focus on the business and the domain of the problem when building features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Do You Engineer Features?\n",
    "* Numeric data\n",
    "* Categorical data\n",
    "* Text data\n",
    "* Temporal data\n",
    "* Image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numeric data\n",
    "* Values\n",
    "* Counts\n",
    "* Binaraization\n",
    "* Rounding\n",
    "* Interactions\n",
    "    * PolynomialFeatures\n",
    "* Binning\n",
    "    * Fixed-Width Binning\n",
    "        * pd.cut(.., bins=.., labels=..)\n",
    "    * Adaptive Binning\n",
    "        * Quantile-Based\n",
    "            * pd.qcut()\n",
    "* Statistical Transformations\n",
    "    * Log Transform\n",
    "    * Box-Cox Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical data\n",
    "* Transforming Nominal Features\n",
    "    * Label encoder\n",
    "* Transforming Ordinal Features\n",
    "    * Map function\n",
    "* One Hot Encoding scheme\n",
    "* Dummy Coding scheme (n-1)\n",
    "* Effect Coding scheme (replace all 0 to -1 for dummy coding)\n",
    "* Bin-counting scheme e.g. IP address\n",
    "* Feature Hashing scheme e.g. FeatureHasher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data\n",
    "* Pre-processing and normalizing text\n",
    "    * Text Tokenization and lower casing\n",
    "    * Removing special characters\n",
    "    * Contraction expansion\n",
    "    * Removing stopwords\n",
    "    * Correcting spellings\n",
    "    * Stemming\n",
    "    * Lemmatization\n",
    "* Feature extraction and engineering\n",
    "* Bag of words model\n",
    "* Bag of n-grams model\n",
    "* TF-IDF Model (Term Frequency Inverse Document Frequency)\n",
    "* Document Similarity e.g. cosine distance, BM25 distance, Hellinger-Bhattacharya distance, jaccard distance\n",
    "* Topic Model\n",
    "* Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal data\n",
    "* Date-based features\n",
    "    * Year, Month, Day, DayOfWeek, DayName, DayOfYear, WeekOfYear, Quarter\n",
    "* Time-based features\n",
    "    * Hour, Minute, Second, MicroSecond, UTC_offset\n",
    "    * TimeOfDayBin : \n",
    "        * [-1,5,11,16,21,23] \n",
    "        * ['Late Night', 'Morning', 'Afternoon', 'Evening', 'Night']\n",
    "* Elapsed time difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image data\n",
    "* Image metadata features\n",
    "    * image create date and time\n",
    "    * image dimensions\n",
    "    * image compression format\n",
    "    * device make and model\n",
    "    * image resolution and aspect ratio\n",
    "    * image artist\n",
    "    * flash, aerture, focal length, and exposure\n",
    "* Raw Image and Channel Pixels\n",
    "    * RGB\n",
    "* Grayscale image pixels\n",
    "    * Y = 0.2125*R + 0.7154*G + 0.0721*B  [0,1] [black, white]\n",
    "* Binning Image Intensity Distribution\n",
    "* Image aggregation statistics\n",
    "    * describe\n",
    "* Edge Detection\n",
    "    * sklearn - canny\n",
    "* Object detection\n",
    "    * sklearn - hog\n",
    "* Localized feature extraction\n",
    "    * mahotas.feature.surf\n",
    "* Visual Bag of Words Model\n",
    "* Automated Feature Engineering with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature scaling\n",
    "- from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "- Standardized Scaling\n",
    "    - mean :0, var : 1\n",
    "- Mix-Max Scaling\n",
    "- Robust Scaling\n",
    "    * (X - median) / IQR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "* Filter methods\n",
    "    * Threshold-base Methods e.g. word frequency\n",
    "    * Statistical methods\n",
    "        * mutual information\n",
    "        * ANOVA (analysis of variance)\n",
    "        * chi-square test\n",
    "        * SelectKBest chi2\n",
    "        * For regression: f_regression, mutual_info_regression\n",
    "        * For classification: chi2, f_classif, mutual_info_classif\n",
    "* Wrapper methods\n",
    "    * Recursive Feature Elimination\n",
    "* Embedded methods\n",
    "    * Model-Based Selection e.g. feature importance of random forests, decision trees\n",
    "* Dimensionality Reduction\n",
    "    * PCA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
